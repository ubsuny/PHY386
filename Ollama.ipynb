{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMBRGmHhh2L0UPLgZ01B42o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubsuny/PHY386/blob/Examples/Ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running a Local LLM on Google Colab\n",
        "\n",
        "This guide explains how to run a local LLM (Large Language Model) on Google Colab using [ollama](https://ollama.com/)."
      ],
      "metadata": {
        "id": "9nY8arHDq3vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0: Enable GPU acceleration on google colab\n",
        "\n",
        "If you are using Google Colab, you can speed up the LLM by enabling GPU acceleration. To do this:\n",
        "\n",
        "1. Click on **Runtime** in the top menu.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, choose **GPU**.\n",
        "4. Click **Save**."
      ],
      "metadata": {
        "id": "qxtcgpAitIqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install and Load xTerm to install and run ollama in Step 2\n",
        "\n",
        "- `!pip install colab-xterm`: Installs the `colab-xterm` package, which allows running an interactive terminal inside Colab.\n",
        "- `%load_ext colabxterm`: Loads the xTerm extension to enable terminal access."
      ],
      "metadata": {
        "id": "TNof8EyGrMGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HOVBikl3lcF"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dnQ1Kq0wrdSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Install and run Ollama\n",
        "\n",
        "The next cell will run a terminal in a code cell (`%xterm`). Run the following comands one after the other in the terminal in the cell:\n",
        "\n",
        "1. `apt install lshw` (Install lshw that checks if a GPU is installed)\n",
        "2. `curl -fsSL https://ollama.com/install.sh | sh` (install Ollama)\n",
        "3. `ollama serve &` (runs Ollama as a service, hit return again after this one to get back to the prompt)\n",
        "4. `ollama pull llama2` (pull your favorite LLM, for examples see [here](https://ollama.com/search))"
      ],
      "metadata": {
        "id": "BJaEgGmT5yv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "b0Z9chW23oO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Install Required Packages\n",
        "- Installs the langchain_community package, which provides integration with various language models, including Ollama."
      ],
      "metadata": {
        "id": "bfLRtUZZrogI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "CNVagGEQ3qPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### set environment variables\n",
        "Imports the os module to modify environment variables.\n",
        "Sets OLLAMA_HOST to 127.0.0.1:11434, which is the local address where the Ollama server will run."
      ],
      "metadata": {
        "id": "AxT1tacLsvQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OLLAMA_HOST'] = '127.0.0.1:11434'"
      ],
      "metadata": {
        "id": "GgBc4aUt3vLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run your own prompt on the local model\n",
        "\n",
        "- Imports the Ollama module from langchain_community.llms.\n",
        "- example prompt, change it to your prompt\n",
        "- Initializes the Ollama model using the llama2 model.\n",
        "- Invokes the model with a query\n",
        "- Prints the model's response."
      ],
      "metadata": {
        "id": "UdndsV--sYWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "\n",
        "prompt = \"I want to expand the physics model of coupled harmonic oscillators. \\\n",
        "I have a chain of masses coupled with springs. \\\n",
        "Give me a creative way how to couple those masses and show me how to calculate their Eigenfrequencies \\\n",
        "in a Jupyter notebook. Then ask me if that model makes sense in physics. \\\n",
        "Take my answer and adapt the model accordingly updating the code. \\\n",
        "Repeat this till Iâ€™m confident that you suggested a model that can exist in the real world.\"\n",
        "# Initialize the Ollama model\n",
        "llm = Ollama(model=\"llama2\")\n",
        "# Generate a response\n",
        "response = llm.invoke(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "SJJORfL34RIp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}